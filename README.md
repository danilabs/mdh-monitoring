# MDH Monitoring - Million Dollar Homepage Analysis & Monitoring

[![Code Quality & Security](https://github.com/danilabs/mdh-monitoring/actions/workflows/ci.yml/badge.svg)](https://github.com/danilabs/mdh-monitoring/actions/workflows/ci.yml)
[![Daily Domain Analysis](https://github.com/danilabs/mdh-monitoring/actions/workflows/daily-domain-analysis.yml/badge.svg)](https://github.com/danilabs/mdh-monitoring/actions/workflows/daily-domain-analysis.yml)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**A comprehensive monitoring and analysis tool for the Million Dollar Homepage** - Track pixel data, monitor domain health, and analyze historical changes over time.

## ğŸ¯ Overview

MDH Monitoring is a sophisticated Python toolkit designed to continuously monitor and analyze the Million Dollar Homepage (http://www.milliondollarhomepage.com/). This project goes beyond simple data extraction to provide comprehensive domain health monitoring, historical trend analysis, and automated reporting capabilities.

**Key Capabilities:**
- **Pixel Data Extraction**: Parse HTML map elements to extract detailed pixel area information
- **Domain Health Monitoring**: Automated DNS, HTTP, and WHOIS status checking for all domains
- **Historical Tracking**: Track changes over time with timestamped data snapshots
- **Automated Reporting**: Generate comprehensive markdown and JSON reports
- **CI/CD Integration**: Automated daily analysis with GitHub Actions workflows

## Features

- Downloads the Million Dollar Homepage HTML
- Extracts pixel area data from HTML map elements
- Generates comprehensive JSON output with metadata and statistics
- Calculates domain statistics and pixel distributions
- Provides timestamped output files
- **Domain Analysis**: Analyzes domains for DNS status, HTTP availability, and WHOIS registration
- **Comprehensive Reports**: Generates detailed domain analysis reports with status summaries

## Installation

```bash
git clone https://github.com/danilabs/mdh-monitoring.git
cd mdh-monitoring
pip install -r requirements.txt
```

### Development Installation

For development with all testing and linting tools:

```bash
pip install -r requirements.txt -r requirements-dev.txt
```

Or use the provided Makefile:

```bash
make dev-setup
```

## Usage

### Basic Pixel Analysis

```bash
python -m mdh_analyzer
```

This will:
1. Download the Million Dollar Homepage as `web.html`
2. Extract pixel data from the HTML map areas
3. Generate a timestamped JSON file with the format `pixel_data_{timestamp}.json`

### Domain Analysis

Analyze domains from existing pixel data:

```bash
python -m mdh_analyzer analyze-domains data/pixel_data_20251112_202426.json
```

Or use the Python API:

```python
from mdh_analyzer import DomainAnalyzer

analyzer = DomainAnalyzer()
domains = analyzer.extract_domains_from_json("data/pixel_data_20251112_202426.json")
results = [analyzer.analyze_domain(domain) for domain in domains]
analyzer.save_domain_report(results)
```

### Advanced Usage

```python
from mdh_analyzer import MillionDollarAnalyzer

analyzer = MillionDollarAnalyzer()
data = analyzer.analyze()
analyzer.save_json(data, "custom_output.json")
```

### Command Line Interface

The package provides a comprehensive CLI:

```bash
# Analyze pixels with custom URL and output directory
python -m mdh_analyzer analyze-pixels --url http://example.com --output-dir custom_data/

# Analyze domains with verbose output
python -m mdh_analyzer domains data/pixel_data.json --output-dir reports/ --verbose

# Generate markdown report from pixel and domain data
python -m mdh_analyzer report data/pixel_data.json reports/report_20251112_205048.json

# Get help
python -m mdh_analyzer --help
```

## Output Format

### Pixel Data Output

The generated JSON file contains:

- **metadata**: Information about the extraction process
- **summary_statistics**: Aggregate statistics about domains and pixels
- **domains**: Detailed breakdown of each domain with their pixel areas

Example structure:
```json
{
  "metadata": {
    "generated_at": "2025-11-12T19:57:49.506022",
    "source_file": "web.html",
    "description": "Million Dollar Homepage pixel data extraction",
    "total_pixels": 1002800,
    "total_unique_domains": 2822,
    "total_areas": 3306
  },
  "summary_statistics": {
    "largest_domain_by_pixels": "Pending Order",
    "largest_domain_pixels": 18900,
    "average_pixels_per_domain": 355.35,
    "average_pixels_per_area": 303.33
  },
  "domains": [...]
}
```

### Domain Analysis Output

Domain analysis reports are saved to the `reports/` folder with the format `report_{timestamp}.json`:

```json
{
  "metadata": {
    "generated_at": "2025-11-12T20:15:30.123456",
    "description": "Million Dollar Homepage domain analysis",
    "total_domains": 2795
  },
  "summary_statistics": {
    "total_domains": 2795,
    "dns_noerror": 1234,
    "dns_nxdomain": 987,
    "dns_timeout": 45,
    "dns_error": 529,
    "http_success": 1156,
    "http_client_error": 234,
    "http_server_error": 89,
    "http_timeout": 567,
    "http_connection_error": 749,
    "whois_registered": 1890,
    "whois_available": 456,
    "whois_unknown": 449
  },
  "domains": [
    {
      "domain": "example.com",
      "dns_status": "NOERROR",
      "http_status": 200,
      "whois_status": "registered"
    }
  ]
}
```

#### Status Codes Explained

**DNS Status:**
- `NOERROR`: Domain resolves successfully
- `NXDOMAIN`: Domain does not exist
- `TIMEOUT`: DNS query timed out
- `ERROR`: Other DNS resolution error

**HTTP Status:**
- `200`, `301`, `404`, etc.: Standard HTTP status codes
- `TIMEOUT`: HTTP request timed out
- `CONNECTION_ERROR`: Could not connect to server
- `SSL_ERROR`: SSL/TLS certificate error

**WHOIS Status:**
- `registered`: Domain is registered
- `available`: Domain is available for registration
- `unknown`: Could not determine registration status

### Markdown Report Output

Comprehensive markdown reports are generated combining pixel data and domain analysis:

```markdown
# Million Dollar Homepage - Domain Analysis Report

**Analysis Date:** November 12, 2025 at 19:50 UTC
**Source Data:** web.html

## ğŸ“Š Executive Summary

This report analyzes **2,795 unique domains** found across **1,002,800 pixels** in **3,306 areas** of the Million Dollar Homepage.

### Key Findings:

- ğŸŸ¢ **1,890 domains (67.6%)** are registered and potentially active
- ğŸ”´ **456 domains (16.3%)** no longer exist (NXDOMAIN)
- ğŸŸ¡ **449 domains (16.1%)** are available for purchase
- ğŸ’° **125,400 pixels** are associated with available domains

## ğŸ›’ Available Domains for Purchase

**449 domains** are available for purchase, representing **125,400 pixels** of the Million Dollar Homepage.

### Top Available Domains by Pixel Count

| Domain | Pixels | DNS Status | HTTP Status |
|--------|--------|------------|-------------|
| `example-domain.com` | 2,500 | NXDOMAIN | 0 |
| `another-available.net` | 1,800 | NXDOMAIN | 0 |
```

The markdown reports include:
- **Executive Summary**: Key statistics and findings
- **Domain Status Analysis**: Detailed breakdowns by DNS, HTTP, and WHOIS status
- **Available Domains**: Sorted by pixel count for investment opportunities
- **NXDOMAIN Analysis**: Domains that no longer exist
- **Top Active Domains**: Largest registered domains still active
- **Technical Details**: Analysis methodology and data sources

## ğŸ“ Project Structure

```
mdh-monitoring/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yml                         # Code quality & security pipeline
â”‚       â””â”€â”€ daily-domain-analysis.yml      # Automated daily domain analysis
â”œâ”€â”€ data/
â”‚   â””â”€â”€ pixel_data_YYYYMMDD_HHMMSS.json   # Historical pixel data snapshots
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ report_YYYYMMDD_HHMMSS.json       # Domain analysis data (JSON)
â”‚   â””â”€â”€ report_YYYYMMDD_HHMMSS.md         # Human-readable reports (Markdown)
â”œâ”€â”€ mdh_analyzer/                          # Core analysis package
â”‚   â”œâ”€â”€ __init__.py                        # Package initialization
â”‚   â”œâ”€â”€ __main__.py                        # CLI entry point
â”‚   â”œâ”€â”€ analyzer.py                        # Core pixel analysis engine
â”‚   â”œâ”€â”€ cli.py                             # Command-line interface
â”‚   â”œâ”€â”€ domain_analyzer.py                 # Domain health monitoring
â”‚   â”œâ”€â”€ downloader.py                      # Web content retrieval
â”‚   â”œâ”€â”€ parser.py                          # HTML parsing utilities
â”‚   â””â”€â”€ report_generator.py                # Report generation
â”œâ”€â”€ tests/                                 # Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_analyzer.py                   # Pixel analysis tests
â”‚   â””â”€â”€ test_domain_analyzer.py            # Domain analysis tests
â”œâ”€â”€ requirements.txt                       # Production dependencies
â”œâ”€â”€ requirements-dev.txt                   # Development dependencies
â”œâ”€â”€ setup.py                               # Package configuration
â”œâ”€â”€ pytest.ini                             # Test configuration
â”œâ”€â”€ Makefile                               # Development automation
â”œâ”€â”€ README.md                              # Project documentation
â”œâ”€â”€ LICENSE                                # MIT License
â”œâ”€â”€ STATS.md                               # Project statistics
â””â”€â”€ .gitignore                             # Git ignore rules
```

## Requirements

- Python 3.7+
- requests
- beautifulsoup4
- lxml
- dnspython (for domain analysis)
- python-whois (for domain analysis)

## ğŸ¤– Automated Analysis Workflows

MDH Monitoring includes sophisticated GitHub Actions workflows for continuous monitoring and quality assurance:

### 1. ğŸ” Code Quality & Security Pipeline (`ci.yml`)
**Comprehensive quality assurance** that runs on every push and pull request to ensure code reliability and security.

**Pipeline Components:**
- **ğŸ§ª Unit Tests & Coverage**: Pytest with coverage analysis and Codecov integration
- **ğŸ“ Code Quality Analysis**: Pylint with configurable scoring (minimum 7.0/10)
- **ğŸ”’ Security Vulnerability Assessment**: Bandit security scanning with critical issue blocking
- **ğŸ”¤ Static Type Analysis**: MyPy type checking for better code documentation
- **ğŸ“‹ Automated PR Annotations**: Inline code review comments for quality issues

**Quality Standards:**
- Minimum test coverage requirements
- No high/critical security vulnerabilities allowed
- Consistent code formatting and style enforcement
- Cross-platform compatibility testing

### 2. ğŸŒ Automated Daily Domain Analysis (`daily-domain-analysis.yml`)
**Intelligent domain health monitoring** that automatically analyzes all domains from the latest pixel data.

**Smart Features:**
- **â° Scheduled Execution**: Runs daily at 2:00 AM UTC for consistent monitoring
- **ğŸ” Auto-Discovery**: Automatically locates the most recent pixel data file
- **âš¡ High-Performance Processing**: Multi-threaded analysis with 20 concurrent workers
- **ğŸ“Š Comprehensive Health Checks**: DNS resolution, HTTP availability, WHOIS registration
- **ğŸ“ Dual Report Generation**: Both JSON data files and human-readable markdown reports
- **ğŸ’¾ Automated Commit**: Results automatically committed to repository with detailed metadata
- **ğŸ¯ Manual Trigger Support**: Can be manually executed from GitHub Actions interface

**Analysis Capabilities:**
- DNS status verification (NOERROR, NXDOMAIN, timeouts)
- HTTP availability testing with status code analysis
- WHOIS registration status checking
- Progress tracking with detailed execution summaries
- Error handling and timeout management (15-second timeout per domain)

### Viewing Historical Data:
- **Pixel Data**: Check the `data/` folder for historical pixel analysis files
- **Domain Reports**: Check the `reports/` folder for domain analysis reports (JSON data and markdown summaries)
- Each file contains a complete snapshot with timestamps for tracking changes over time
- Markdown reports provide human-readable summaries with available domains sorted by pixel value

## Development

### Local Development Commands

This project includes a Makefile for easy development:

```bash
# Set up development environment
make dev-setup

# Run all CI checks locally
make ci

# Individual commands
make test              # Run tests with coverage
make lint              # Run pylint
make security          # Run security scans
make type-check        # Run type checking
make format            # Format code with black and isort
make clean             # Clean up generated files

# Quick development workflow
make quick-check       # Run lint + test
make pre-commit        # Run all pre-commit checks
```

### Code Quality Standards

This project maintains high code quality standards:

- **Pylint Score**: Minimum 7.0/10
- **Test Coverage**: Minimum 70%
- **Security**: No high/critical vulnerabilities allowed
- **Type Hints**: Encouraged for better code documentation
- **Code Formatting**: Black and isort for consistent style

## Domain Analysis Features

The domain analyzer provides comprehensive analysis of all domains found in the pixel data:

### DNS Resolution Check
- Verifies if domains resolve to IP addresses
- Identifies non-existent domains (NXDOMAIN)
- Handles DNS timeouts and errors

### HTTP Status Check
- Performs HEAD requests to check website availability
- Returns standard HTTP status codes (200, 404, 500, etc.)
- Handles connection errors, timeouts, and SSL issues
- Tests both HTTP and HTTPS protocols

### WHOIS Registration Check
- Determines if domains are registered or available
- Provides registration status information
- Handles WHOIS query errors gracefully

### Use Cases
- **Domain Portfolio Analysis**: Understand which domains are still active
- **Website Availability**: Check which Million Dollar Homepage links still work
- **Domain Research**: Identify available domains from the historical data
- **Data Quality**: Validate domain data integrity

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## License

MIT License - see LICENSE file for details.

## About the Million Dollar Homepage

The Million Dollar Homepage was created by Alex Tew in 2005 as a way to raise money for university. The website consists of a 1000Ã—1000 pixel grid, with each pixel sold for $1. This analyzer helps understand the distribution and organization of these pixel purchases.